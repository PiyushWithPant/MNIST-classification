{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistData, mnistInfo = tfds.load(\n",
    "    name='mnist',   # dataset name\n",
    "    as_supervised=True, # loads the dataset as a 2 tuple structure (input, target)\n",
    "    with_info= True, # mnist data info\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistTrain, mnistTest = mnistData['train'], mnistData['test']  \n",
    "\n",
    "# out of 70K data, 60K is training, 10k is testing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation dataset\n",
    "\n",
    "num_train = mnistInfo.splits['train'].num_examples   # 60000\n",
    "\n",
    "num_Validation = 0.1 * num_train\n",
    "\n",
    "\n",
    "# type(num_Validation) # it is a float\n",
    "\n",
    "# so now we will convert it to an int\n",
    "\n",
    "num_Validation = tf.cast(num_Validation,tf.int64)\n",
    "\n",
    "type(num_Validation)\n",
    "\n",
    "\n",
    "num_Test = mnistInfo.splits['test'].num_examples \n",
    "\n",
    "num_Test = tf.cast(num_Test,tf.int64)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32 ) # ensuring all data is of same datatype\n",
    "    \n",
    "    # since each color is between 0 to 255 range, so if we divide each num with 255, we will get same scale\n",
    "    \n",
    "    image /=255.    # dot means float result\n",
    "    \n",
    "    return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTrainAndValidationData = mnistTrain.map(scale)\n",
    "scaledTestData = mnistTest.map(scale)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we will use mini-batch, we need to shuffle each batch or the dataset itself\n",
    "BUFFER_SIZE = 10000\n",
    "# since our dataset is huge, so we will not shuffle whole data at once, since it will be memory wise expensive\n",
    "# so we will shuffle 10k data at once and then next\n",
    "\n",
    "shuffled_train_and_validation_data = scaledTrainAndValidationData.shuffle(BUFFER_SIZE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Validation and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we got the number of validation data ie. 6k, we will extract it from the dataset now\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_Validation)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_Validation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO-\n",
    "\n",
    "SGD => BATCH SIZE = 1\n",
    "\n",
    "BATCH GD => BATCH SIZE = NO. OF SAMPLES\n",
    "\n",
    "MINI-BATCH => 1 < BATCH SIZE < NO. OF SAMPLES\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_Validation)\n",
    "test_data = scaledTestData.batch(num_Test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "# Next loads the next batch\n",
    "# Iter makes the object iterable one element at a time like for loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 784 inputs (INPUT LAYERS), 10 output nodes as there are 10 digits (OUTPUT LAYERS). We will create 2 HIDDLE LAYERS with 50 nodes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_size = 100 # assumping that all hidden layers are of same size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Flatten(input_shape = (28,28,1)),   # INPUT LAYER\n",
    "    # FLATTEN - it is used to transform a TENSOR of n-rank into a vector\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_size, activation = 'relu'),    # HIDDLE LAYER 1\n",
    "    # this step takes the input and calculate the DOT PRODUCT of the input and weights and adds a bias\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_size, activation = 'relu'),    # HIDDLE LAYER 2\n",
    "    \n",
    "    tf.keras.layers.Dense(output_size, activation= 'softmax')   # OUTPUT LAYER\n",
    "    # SOFTMAX ensures that the output is transformed into probabilites of output\n",
    "    \n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam', # Adaptive Moment Estimator\n",
    "    loss ='sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "# there are many types of crossentropy, \n",
    "# binary CE gives binary form\n",
    "# categorical CE expects that the data is already in ONE HOT ENCODED form\n",
    "# sparse CCE applies one hot encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 [==============================] - 4s 5ms/step - loss: 0.3257 - accuracy: 0.9086 - val_loss: 0.1575 - val_accuracy: 0.9567\n",
      "Epoch 2/10\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.1372 - accuracy: 0.9595 - val_loss: 0.1121 - val_accuracy: 0.9678\n",
      "Epoch 3/10\n",
      "540/540 [==============================] - 3s 4ms/step - loss: 0.0981 - accuracy: 0.9698 - val_loss: 0.0871 - val_accuracy: 0.9742\n",
      "Epoch 4/10\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0765 - accuracy: 0.9764 - val_loss: 0.0773 - val_accuracy: 0.9772\n",
      "Epoch 5/10\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0619 - accuracy: 0.9812 - val_loss: 0.0640 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "540/540 [==============================] - 3s 5ms/step - loss: 0.0502 - accuracy: 0.9850 - val_loss: 0.0572 - val_accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "540/540 [==============================] - 3s 4ms/step - loss: 0.0403 - accuracy: 0.9875 - val_loss: 0.0516 - val_accuracy: 0.9852\n",
      "Epoch 8/10\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0336 - accuracy: 0.9899 - val_loss: 0.0449 - val_accuracy: 0.9863\n",
      "Epoch 9/10\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0295 - accuracy: 0.9912 - val_loss: 0.0401 - val_accuracy: 0.9885\n",
      "Epoch 10/10\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0269 - accuracy: 0.9920 - val_loss: 0.0334 - val_accuracy: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22fc6ca0670>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    validation_data = (validation_inputs, validation_targets),\n",
    "    # verbose = 2\n",
    ")\n",
    "\n",
    "# After training the NN epoch by epoch and passing the train data as batches while updating the weights, this is FORWARD PROPAGRATION\n",
    "# after this is done, the VALIDATION data is passed entirely through the NN as forward propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using 50 nodes in hidden layer, the accuracy of Validation was 97 <br>\n",
    "When using 100 nodes in hidden layer, the accuracy of Validation went to 98 <br>\n",
    "NOTE - Validation accuracy is true accuracy, hence u can see if your model is facing overfitting or not\n",
    "\n",
    "<hr>\n",
    "\n",
    "After adding another hidden layer, the accuracy increased to 98.20\n",
    "\n",
    "<hr>\n",
    "\n",
    "After adding another hidden layer, and increasing nodes to 200, the accuracy actually decreased to 98.15\n",
    "Hence, the optimal value of hidden layer for this problem is 2-3 with nodes from 50-100\n",
    "<hr>\n",
    "\n",
    "After updating EPOCHS = 10, the VALIDATION accuracy reached 99% (one of the highest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 380ms/step - loss: 0.0762 - accuracy: 0.9769\n"
     ]
    }
   ],
   "source": [
    "test_loss , test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss :  0.08%\n",
      "Test Accuracy :  97.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss : {0: .2f}%\\nTest Accuracy : {1: .2f}%\".format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
